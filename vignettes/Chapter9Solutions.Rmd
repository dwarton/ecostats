---
title: "Chapter 9 - Design-based inference - Exercise solutions and Code Boxes"
author: "David Warton"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Chapter 9 - Design-based inference - Exercise solutions and Code Boxes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Exercise 9.1: Smoking in pregnancy
*Consider again the guinea pig experiment... How can we make inferences about the treatment effect, without assuming normality?*

Design-based inference allows us to relax distributional assumptions. Specifically, if we use a permutation test as in Code Box 9.1, or a bootstrap test as in Code Box 9.3, we do not assume normality.

## Exercise 9.2: Three example permutations of treatment labels in the guinea pig data

*Does the observed statistic of 2.67 seem large compared to these values? What does this tell you?*

The three statistics obtained by permuting data were 0.76, -2.09 and -1.08. These are all smaller than 2.67, suggesting that 2.67 is unusually large compared to what we would expect if there were no treatment effect. However, to say something more precise about how unusual this is we should do many more permutations...


## Code behind Figure 9.1

```{r fig91, fig.width=6, fig.height=5}
library(ecostats)
data(guineapig)
ft_guinea = lm(errors~treatment,data=guineapig)
tObs = summary(ft_guinea)$coef[2,3] #store observed t-statistic

nPerm = 1000
tStats = rep(NA,nPerm)
tStats[1] = tObs
for(iPerm in 2:nPerm)
{
  guineapig$treatPerm = sample(guineapig$treatment) #permute treatment labels
  ft_guineaPerm = lm(errors~treatPerm,data=guineapig) #re-fit model
  tStats[iPerm] = summary(ft_guineaPerm)$coef[2,3] #store t-stat
}
par(cex=1.2,lwd=1.5)
hist(tStats,main="Null distribution of t under permutation",xlab="t")
abline(v=tObs,col="red") #put a red line on plot for observed t-stat
p = mean( tStats >= tObs ) #compute P-value
print(p)
```

## Code Box 9.1: A permutation test for the guinea pig data using `mvabund`

```{r box91}
library(mvabund)
data(guineapig)
ft_guinea = manylm(errors~treatment,data=guineapig)
anova(ft_guinea)
```


## Code Box 9.2: Permutation test for a relationship between latitude and plant height

```{r box92}
data(globalPlants)
ft_height = manylm(height~lat, data=globalPlants)
anova(ft_height)
```


## Code Box 9.3: Using the `mvabund` package for a bootstrap test of guinea pig data

```{r box93}
library(mvabund)
ft_guinea = manylm(errors~treatment, data=guineapig)
anova(ft_guinea, resamp="residual")
```


## Exercise 9.3: Case resampling in the guinea pig data

*We can... resample cases... Below are three examples.*

C  N  N  N  C  N  N  C  N  N  C  C  N  N  C  C  N  N  N  N
10 33 28 33 47 66 33 26 63 66 36 20 38 28 35 15 66 33 43 26

N  N  N  N  C  N  N  N  N  N  C  C  C  C  N  C  C  C  N  C
33 89 34 89 35 43 38 33 23 63 10 26 11 10 43 47 10 19 66 35

N  N  C  C  N  N  N  C  N  C  N  C  N  N  N  C  C  C  C  C
66 66 19 47 63 43 43 20 33 15 28 26 89 38 43 47 20 20 11 20

*Count the number of Controls in each case resampled dataset. Did you get the number you expected to?*

OK so the first one has 8 Control sites, second one has 9 Control sites, third has 10 Control sites. This is not expected in the sense that the original study was planned with 10 Control and 10 Treatment guinea pigs, so we have messed with the design by doing case resampling :(


## Exercise 9.4: Global plant height -- does rainfall explain latitude effect?
*She wants to know: Does latitude explain any variation in plant height beyond that due to rainfall?*

*What is the model that should be fitted under the null hypothesis? Does it include any predictor variables?*

The model under the null hypothesis, as in Code Box, includes rainfall:
```{r ex94}
ft_heightR=lm(height~rain, data=globalPlants)
```

and we want to see if there is significant additional variation in plant height that is explained by latitude, beyond that explained by rainfall.


## Code Box 9.4: Residual resampling using `mvabund` for Exercise 9.4.

```{r box94}
ft_heightRL=manylm(height~rain+lat, data=globalPlants)
anova(ft_heightRL, resamp="perm.resid")
```


## Code Box 9.5: Plant height data -- checking assumptions

```{r box95, fig.width=4, fig.height=3.5}
ft_height = lm(height~lat, data=globalPlants)
plotenvelope(ft_height)
```


## Exercise 9.5: Plant height data -- log transformation

Refit the linear model to the plant height data, available as the `globalPlants` dataset in the `ecostats` package, using a log-transformation of the response. Use residual resampling to test for an effect of latitude after controlling for the effect of rainfall.
```{r ex95, fig.width=4, fig.height=3.5}
globalPlants$loght = log(globalPlants$height)
ft_loghtRL=manylm(loght~rain+lat, data=globalPlants)
anova(ft_loghtRL, resamp="perm.resid")
ft_loghtRLlm = lm(loght~rain+lat, data=globalPlants)
plotenvelope(ft_loghtRLlm)
```

*How do results compare to the analysis without a log-transformation?*

Now the test statistics are larger and more significant (which I guess because we can better see the signal in data when we are closer to satisfying our assumptions).

*How do results compare to what you would have got if you used model-based inference, by applying `anova` to the `lm` function? Is this what you expected?*

```{r ex95 lm}
anova(ft_loghtRLlm)
```

Results are nearly identical, which is as expected, because there weren't any violations of distributional assumptions that might have warranted resampling.


## Exercise 9.6: Guinea pig data -- log transformation
*Log-transform number of errors and check assumptions. Does this better satisfy assumptions than the model on untransformed data?*

```{r ex96, fig.width=4, fig.height=3.5}
data(guineapig)
guineapig$logErrors = log(guineapig$errors)
ft_guineaLog = lm(logErrors~treatment,data=guineapig)
plotenvelope(ft_guineaLog)
by(guineapig$logErrors,guineapig$treatment,sd)
```

This looks a *lot* better. Standard deviations are similar now, no obvious non-normal trend on quantile plot.


*Repeat the permutation test of Code Box 9.2 on log-transformed data.*

```{r ex96 anova}
library(mvabund)
ftMany_guineaLog = manylm(logErrors~treatment,data=guineapig)
anova(ftMany_guineaLog)
```

*How do results compare to the analysis without a log-transformation? Is this what you expected to happen?*

Results are similar, but the test statistic is slightly larger and slightly more significant. This is expected because by fitting a model for the data that is closer to satisfying assumptions, it will work better and more clearly see any signal in the data.


## Exercise 9.7: Revisiting linear models past

*Go back to a couple of linear models (with fixed effects terms only) you have previously fitted, e.g. in the exercises of Chapter 4, and reanalyse using (residual) resampling for inference.*

### Repeating Exercise 4.9:
```{r ex49, fig.width=4, fig.height=3.5}
 data(snowmelt)
 snowmelt$logFlow = log(snowmelt$flow)
 snowmelt$logFlow[snowmelt$logFlow==-Inf]=NA
 ft_logsnow = lm(logFlow~elev+snow, data=snowmelt)
 plotenvelope(ft_logsnow)
 summary(ft_logsnow)
 confint(ft_logsnow)
```

Repeating using resampling:
```{r ex49 re}
library(mvabund)
mft_logsnow = manylm(logFlow~elev+snow, data=snowmelt)
summary(mft_logsnow)
confint(mft_logsnow)
```

*Did results work out differently? Is this what you expected? (Think about sample size and the normality assumption.)*

*P*-values are pretty much the same, which is expected, because we didn't have concerns about normality and the sample size was big enough for CLT to deal with such concerns anyway.

The confidence intervals are identical, because they are computed in the same way irrespective of resampling.


### Now repeating Exercise 4.10:
```{r ex410}
data(aphidsBACI)
lm_aphids = lm(logcount~Plot+Time+Treatment:Time,data=aphidsBACI)
anova(lm_aphids)
```

```{r ex10 re}
mlm_aphids = manylm(logcount~Plot+Time+Treatment:Time,data=aphidsBACI)
anova(mlm_aphids)
```

This looks fairly similar, the test statistic is a bit different (but equivalent) and significance levels generally similar. There is slightly more movement in *P*-values than last time, which may be related to the sample size being smaller (in fact the residual degrees of freedom is only `6`.)

## Code Box 9.6: Block resampling using `mvabund` for estuary data


```{r box96}
  data(estuaries)
  ft_estLM = manylm(Total~Mod,data=estuaries)
  anova(ft_estLM,resamp="case",block=estuaries$Estuary)
```


## Code Box 9.7: Block resampling using `permute` for raven data

```{r box97}
data(ravens)
crowGun = ravens[ravens$treatment == 1,]
library(reshape2)
crowLong = melt(crowGun,measure.vars = c("Before","After"),
    variable.name="time",value.name="ravens")

library(permute)
CTRL = how(blocks=crowLong$site)
permIDs = shuffleSet(24,nset=999,control=CTRL)
ravenlm = manylm(ravens~site+time,data=crowLong)
anova(ravenlm,bootID=permIDs)
```

*How do results compare to what we got previously (Code Box 4.2), using model-based inference?*

```{r box97 lm}
ravenlm = lm(ravens~site+time,data=crowLong)
anova(ravenlm)
```

We are getting almost the same *P*-value for the `time` effect. Interestingly, we get a very different *P*-value for the `site` effect. This is because the resampling strategy we used permuted data within sites but did not permute data across sites -- this does not give a valid test for a `site` effect, hence the site *P*-value from the permutation test is not valid.


## Code Box 9.8: Moving block bootstrap test for species richness modelling
```{r box98}
data(Myrtaceae)
# fit a lm:
Myrtaceae$logrich=log(Myrtaceae$richness+1)
mft_richAdd = manylm(logrich~soil+poly(TMP_MAX,degree=2)+
                      poly(TMP_MIN,degree=2)+poly(RAIN_ANN,degree=2),
                                         data=Myrtaceae)
                                         
# construct a boot ID matrix: 
BootID = BlockBootID(x = Myrtaceae$X, y = Myrtaceae$Y, block_L = 20,
             nBoot = 199, Grid_space = 5)
anova(mft_richAdd,resamp="case",bootID=BootID)
```

*How does this compare to what you would get if you just used model-based inference, using the `lm` function?*

```{r box98 lm}
ft_richAdd = lm(logrich~soil+poly(TMP_MAX,degree=2)+
                      poly(TMP_MIN,degree=2)+poly(RAIN_ANN,degree=2),
                                         data=Myrtaceae)
anova(ft_richAdd)
```

Everything is highly significant here, but much less so when using the moving block bootstrap. This is probably partly because the data are spatially autocorrelated, so we are seeing false significance in the `lm` results because dependence of observations hasn't been accounted for. But the difference in results is quite extreme (when considering that the spatial signal wasn't super-strong) so the moving block bootstrap maybe is being overly conservative here too.


## Code Box 9.9: Moving block bootstrap standard errors for species richness predictions
```{r box99}
ft_richAdd = lm(logrich~soil+poly(TMP_MAX,degree=2)+
          poly(TMP_MIN,degree=2)+poly(RAIN_ANN,degree=2),
          data=Myrtaceae)
nBoot=199
predMat = matrix(NA,length(Myrtaceae$logrich),nBoot)
for(iBoot in 1:nBoot)
{
   ids = BootID[iBoot,]
   ft_i = update(ft_richAdd,data=Myrtaceae[ids,])
   predMat[ids,iBoot] = predict(ft_i)
 }
bootSEs = apply(predMat,1,sd,na.rm=TRUE)
lmSEs   = predict(ft_richAdd,se.fit=TRUE)$se.fit
cbind(bootSEs,lmSEs)[1:10,]
```

*Is this what you expected to see?*

The bootstrapped standard errors account for spatial autocorrelation in the data, whereas the `lm` standard errors are not. The effect of ignoring spatial autocorrelation tends to be to have false confidence in your results -- *P*-values and standard errors too small. *P*-values were too small in Code Box 9.8, and here we see standard errors tend to be too small (although a couple are bigger). On average, they are about 25% bigger (which is less than I expected given what happened to the *P*-values).


## Exercise 9.8: Does block length matter?
*For Ian's species richness data, compute standard errors using a few different block sizes (say 5km, 10km, 20km, 40km) and compare.*


This will take a while to run -- the first three lines will take about three times as long as the relevant line from Code Box 9.8 did :(

```{r ex98}
bootSEall = matrix(NA,length(bootSEs),4)
block_Ls=c(5,10,20,40)
colnames(bootSEall)=block_Ls
bootSEall[,3]=bootSEs #we already did block length 20 in Code Box 9.9
for (iLength in c(1,2,4))
{
  BootIDi = BlockBootID(x = Myrtaceae$X, y = Myrtaceae$Y, block_L = block_Ls[iLength],
             nBoot = 199, Grid_space = 5)
  predMat = matrix(NA,length(Myrtaceae$logrich),nBoot)
  for(iBoot in 1:nBoot)
  {
     ids = BootIDi[iBoot,]
     ft_i = update(ft_richAdd,data=Myrtaceae[ids,])
     predMat[ids,iBoot] = predict(ft_i)
   }
  bootSEall[,iLength] = apply(predMat,1,sd,na.rm=TRUE)
}
head(bootSEall)
```

*Did results change, as you changed block size?*

Yep, and standard errors tend to be getting a bit bigger as block size increases

*Compute the mean of the standard errors at each block size. Is there a trend?*
```{r ex98 mean}
apply(bootSEall,2,mean,na.rm=TRUE)
```

The mean of the standard errors of predictions tends to be getting bigger as block size increases. This makes sense because increasing block size effectively reducing sample size, by reducng the number of units in the spatial domain that are considered as independent.

